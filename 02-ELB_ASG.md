# Load Balancing
* LBs are servers that forward internet traffic to multiple servers (EC2 instances) downstream
* Used to spread load across multiple downstream instances
* Expose a single point of access (DNS) to your app.
* Seamleassly handle failures of downstream instances
* Do regular health checks to your instances
* Provide SSL termination (HTTPS) for your websites
* Enforce stickiness with cookies
* High availability across zones
* Separate public traffic from private
* AWS guarantees it will be working
* AWS takes care of upgrades, maintenance, high availability
* AWS provides a few configurations knos
* Integrated with many AWS offerings / services

## Health Checks
* They enable the load balancer to know if instances it forwards traffic to are available to reply to requests
* The health check is done on a port and route (/health is common)
* If the response is not 200 (OK), then the instance is unhealthy

## Types of LBs
* Classic LB (old gen)
  - Deprecated
* Application LB (new gen)
  - Work at Layer 7
  - Load balancing to multiple HTTP apps across machines (target groups)
  - Load balancing to multiple applications on the same machine (containers)
  - Load balancing base on route in URL
  - Load balancing base on the hostname in URL
  - Has a port mapping feature to redirect to a dynamic port
  - In comparison, we would need to create one Classic Load Balancer per app. Expensive and inefficent
  - Stickiness can be enabled at the target group level
    - same request goes to the same instance
    - stickiness is directly generated by the ALB (not the app)
  - Supports HTTP/HTTPS & Websockets protocols
  - App servers don't see the IP of the client directly
    - the true IP of the client is inserted in the header _X-Forwarded-For_
    - Can also get Port (_X-Forwarded-Port_) and proto (_X-Forwared-Proto_)
  - Cannot balance load based on geography
* Network LB (new gen)
  - Work at Layer 4
  - Forward TCP traffic to your instances
  - Handles millions of requests per seconds
  - Support for static IP or elastic IP
  - Less latency ~100 ms (vs 400ms for ALB)
  - Mostly used for extreme performance - not the default choice
  - Can directly see the client IP

## LB Stickiness
* Same client is always redirected to the same instance behind a load balancer
* Works for CLB and ALB
* The "cookie" used for stickiness has an expiration date you control
* Use case: make sure user doesn't lose session data

### Good to know
* CLB and ALB support SSL certificates and provide SSL termination
* All LBs have health check capability
* ALB can route based on hostname / path
* ALL LBs have a static hostname. Do not resolve and use underlying IP
* LBs can scale but not instantaneously
* 4XX errors are client induced errors
* 5XX errors are application induced errors
  - Load Balancers Errors 503 mean at capacity or no registered target

## LBs for Solutions Architect
* CLB: questions on SGs, stickiness
* ALB: Layer 7 of OSI
  - support routing based on hostname (users.example.com & payments.example.com)
  - support routing based on path (example.com/users & example.com/payments)
  - support redirects (from HTTP to HTTPS for example)
  - support dynamic host port mapping with ECS
* NLB: Layer 4 of OSI
  - gets a static IP per AZ
  - public facing: must attach EIP - can help whitelist by clients
  - private facing: will get random private IP based on free ones at time of creation
  - has cross zone load balancing (can route traffic among AZs)
  - SSL termination
* LB SGs:
  - SG for ALB (80 or 443) and then SG for EC2 instance allowing ALB SG (source sg-XXX)
* LB SSL:
  - Uses and X.509 certificate (SSL/TLS server certificate)
  - Can manage certificates using ACM (AWS Certificate Manager)
  - Can create or upload own certificates 
  - HTTPS Listener:
    - must specify default certificate
    - can add optional list of certificates to support multiple domains
    - clients can use SNI (Server Name Indication) to specify the hostname they reach


# AutoScaling Groups
* Scale out: add EC2 instances to match an increased load
* Scale in: remove EC2 instances to match a decreased load
* Ensure we have a minimum and a maximum number of machines running
* Automatically register new instances to a load balancer

## ASG Attributes
* Launch configuration: AMI + Instance type
* EC2 User Data
* EBS Volume
* Security groups
* SSH Key pair
* Min Size / Max Size / Initial Capacity
* Network + Subnets information
* Load Balancer information
* Scaling policies

## AutoScaling Alarms
* Possible to scale an ASG based on CloudWatch alarms
  - an alarm monitors a metric (average cpu for example)
  - Metric are computed for the overall ASG instances
  - Based on the alarm we can create scale-out policies or scale-in policies
* AutoScaling New Rules:
  - better auto scaling rules directly managed by EC2
  - Target Average CPU usage
  - Number of requests on the ELB per instance
  - Average network in
  - Average network out
* AutoScaling custom metric
  - we can auto scale based on a custom metric (eg: number of connected users)
  - Send custom metric from Application on EC2 to CloudWatch (PUTMetric API)
  - Create CloudWatch alarm to react to low / high values
  - Use the CloudWatch alarms as the scaling policy for ASG
* IAM roles attached to an ASG will get assigned to EC2 instances
* ASG are free. You pay for the resources launched
* Instances under ASG mean if they get terminated the ASG will restart them
* ASG can terminate instances marked unhealthy by an LB and replace it

## ASG for Solutions Architect
* ASG default termination policy
  - Find the AZ which has the most number of instances
  - If there are multiple instances in the AZ, delete the one with the oldest launch configuration
  - ASG tries to balance the numbers of instances across AZ.
* Scaling cooldowns
  - helps to ensure your ASG doesnt launch or terminate additional instances before the previous scaling activity takes
    effect
  - We can create cooldowns that apply to a specific simple scaling policy
  - A scaling-specific cooldown period overrides the default cooldown period
  - Default cooldown period of 300 secs can be too long > reduce costs by applying a scaling specific cooldown period of
    180 secs to the scale in policy
  - If your application is scaling up and down multiple times each hour, modify the ASGs cool-down timers and the
    cloudwatch alarm period that triggers the scale in
